{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU3zWuyT8fOH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "\n",
        "# -------------------- Metrics --------------------\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth: float = 1e-6):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    return (2.0 * intersection + smooth) / (\n",
        "        tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth\n",
        "    )\n",
        "\n",
        "\n",
        "def iou_coef(y_true, y_pred, smooth: float = 1e-6):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) - intersection\n",
        "    return (intersection + smooth) / (union + smooth)\n",
        "\n",
        "\n",
        "# -------------------- Building blocks --------------------\n",
        "\n",
        "def conv_bn_relu(x, filters: int, k: int = 3, name: str | None = None):\n",
        "    x = layers.Conv2D(filters, k, padding=\"same\", use_bias=False, name=None if name is None else name+\"_conv\")(x)\n",
        "    x = layers.BatchNormalization(name=None if name is None else name+\"_bn\")(x)\n",
        "    x = layers.ReLU(name=None if name is None else name+\"_relu\")(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def MSFA(x, out_filters: int, name: str | None = None):\n",
        "    \"\"\"Multi-Scale Feature Aggregation (1x1,3x3,5x5,7x7 -> concat).\n",
        "    The sum of branch channels equals out_filters.\n",
        "    \"\"\"\n",
        "    b = max(out_filters // 4, 1)\n",
        "    p1 = conv_bn_relu(x, b, k=1, name=None if name is None else name+\"_1x1\")\n",
        "    p3 = conv_bn_relu(x, b, k=3, name=None if name is None else name+\"_3x3\")\n",
        "    p5 = conv_bn_relu(x, b, k=5, name=None if name is None else name+\"_5x5\")\n",
        "    p7 = conv_bn_relu(x, b, k=7, name=None if name is None else name+\"_7x7\")\n",
        "    x = layers.Concatenate(name=None if name is None else name+\"_concat\")([p1, p3, p5, p7])\n",
        "    return x\n",
        "\n",
        "\n",
        "def AttentionBlock(x, name: str | None = None):\n",
        "    \"\"\"Channel attention (GAP+GMP -> concat -> 1x1 conv -> sigmoid -> scale).\"\"\"\n",
        "    c = x.shape[-1]\n",
        "    gap = layers.GlobalAveragePooling2D(name=None if name is None else name+\"_gap\")(x)\n",
        "    gmp = layers.GlobalMaxPooling2D(name=None if name is None else name+\"_gmp\")(x)\n",
        "    gap = layers.Reshape((1, 1, c), name=None if name is None else name+\"_gap_r\")(gap)\n",
        "    gmp = layers.Reshape((1, 1, c), name=None if name is None else name+\"_gmp_r\")(gmp)\n",
        "    s = layers.Concatenate(axis=-1, name=None if name is None else name+\"_cat\")([gap, gmp])\n",
        "    s = layers.Conv2D(c, 1, padding=\"same\", name=None if name is None else name+\"_conv1x1\")(s)\n",
        "    s = layers.Activation(\"sigmoid\", name=None if name is None else name+\"_sigmoid\")(s)\n",
        "    return layers.Multiply(name=None if name is None else name+\"_scale\")([x, s])\n",
        "\n",
        "\n",
        "def DecoderBlock(x, skip, filters: int, name: str | None = None):\n",
        "    # Upsample\n",
        "    x = layers.Conv2DTranspose(filters, 2, strides=2, padding=\"same\", use_bias=False,\n",
        "                               name=None if name is None else name+\"_up\")(x)\n",
        "    x = layers.BatchNormalization(name=None if name is None else name+\"_up_bn\")(x)\n",
        "    x = layers.ReLU(name=None if name is None else name+\"_up_relu\")(x)\n",
        "\n",
        "    # Attention on upsampled features\n",
        "    x = AttentionBlock(x, name=None if name is None else name+\"_att\")\n",
        "\n",
        "    # Process skip with MSFA and concatenate\n",
        "    if skip is not None:\n",
        "        skip = MSFA(skip, filters, name=None if name is None else name+\"_msfa\")\n",
        "        x = layers.Concatenate(name=None if name is None else name+\"_concat\")([x, skip])\n",
        "\n",
        "    # Refinement with residual\n",
        "    y = conv_bn_relu(x, filters, 3, name=None if name is None else name+\"_conv1\")\n",
        "    y = conv_bn_relu(y, filters, 3, name=None if name is None else name+\"_conv2\")\n",
        "    res = layers.Conv2D(filters, 1, padding=\"same\", name=None if name is None else name+\"_proj\")(x)\n",
        "    out = layers.Add(name=None if name is None else name+\"_add\")([y, res])\n",
        "    return out\n",
        "\n",
        "\n",
        "# -------------------- Model builder --------------------\n",
        "\n",
        "def build_msfa_attention_unet(\n",
        "    input_shape: tuple[int, int, int] = (256, 256, 3),\n",
        "    num_classes: int = 1,\n",
        "    backbone_trainable: bool = False,\n",
        ") -> Model:\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    backbone = tf.keras.applications.MobileNetV2(\n",
        "        include_top=False, input_tensor=inputs, weights=\"imagenet\"\n",
        "    )\n",
        "    backbone.trainable = backbone_trainable\n",
        "\n",
        "    # Feature taps\n",
        "    s1 = backbone.get_layer(\"block_1_expand_relu\").output   # 64x64\n",
        "    s2 = backbone.get_layer(\"block_3_expand_relu\").output   # 32x32\n",
        "    s3 = backbone.get_layer(\"block_6_expand_relu\").output   # 16x16\n",
        "    s4 = backbone.get_layer(\"block_13_expand_relu\").output  # 8x8\n",
        "    b  = backbone.get_layer(\"block_16_project\").output      # 8x8 bottleneck\n",
        "\n",
        "    # Bottleneck MSFA\n",
        "    x = MSFA(b, 512, name=\"bottleneck_msfa\")\n",
        "\n",
        "    # Decoder (4 steps)\n",
        "    x = DecoderBlock(x, s4, 256, name=\"dec4\")  # 8 -> 16\n",
        "    x = DecoderBlock(x, s3, 128, name=\"dec3\")  # 16 -> 32\n",
        "    x = DecoderBlock(x, s2, 64,  name=\"dec2\")  # 32 -> 64\n",
        "    x = DecoderBlock(x, s1, 32,  name=\"dec1\")  # 64 -> 128\n",
        "\n",
        "    # Final upsample to 256x256\n",
        "    x = layers.Conv2DTranspose(32, 2, strides=2, padding=\"same\", name=\"final_up\")(x)\n",
        "\n",
        "    # Output head\n",
        "    activation = \"sigmoid\" if num_classes == 1 else \"softmax\"\n",
        "    outputs = layers.Conv2D(num_classes, 1, activation=activation, name=\"mask\")(x)\n",
        "\n",
        "    return Model(inputs, outputs, name=\"MSFA_Attention_MobileNetV2\")\n",
        "\n",
        "\n",
        "# -------------------- Example usage --------------------\n",
        "if __name__ == \"__main__\":\n",
        "    model = build_msfa_attention_unet(input_shape=(256, 256, 3), num_classes=1, backbone_trainable=False)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[dice_coef, iou_coef, \"accuracy\"],\n",
        "    )\n",
        "    model.summary()\n"
      ]
    }
  ]
}